# advanced_ai_features.py - ê³ ë„í™”ëœ AI ì¶”ì²œ ì‹œìŠ¤í…œ
# TF-IDF, PCA, K-means í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA, LatentDirichletAllocation
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple
import pickle
import os

class UserBehaviorAnalyzer:
    """ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„ ë° í´ëŸ¬ìŠ¤í„°ë§"""
    
    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=100,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.pca = PCA(n_components=10)
        self.kmeans = KMeans(n_clusters=5, random_state=42)
        self.scaler = StandardScaler()
        
        # ì‚¬ìš©ì ìœ í˜• ë¼ë²¨
        self.user_types = {
            0: "ì™„ë²½ì£¼ì˜ì (Early Bird)",
            1: "ê³„íší˜• (Steady Planner)", 
            2: "ë§‰íŒìŠ¤íŒŒíŠ¸ (Last Minute)",
            3: "ì‹ ì¤‘í˜• (Careful Preparer)",
            4: "ì‹¤ìš©ì£¼ì˜ì (Practical Type)"
        }
        
    def create_user_profile(self, user_items: List[dict], behavior_data: dict) -> np.ndarray:
        """ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„±"""
        
        # 1. ì²´í¬ë¦¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ íŠ¹ì„± (TF-IDF)
        item_texts = [f"{item['title']} {item.get('description', '')}" for item in user_items]
        text_features = self.tfidf_vectorizer.fit_transform(item_texts).mean(axis=0).A1
        
        # 2. í–‰ë™ íŒ¨í„´ íŠ¹ì„±
        behavior_features = np.array([
            behavior_data.get('completion_rate', 0.5),
            behavior_data.get('avg_days_early', 0),
            behavior_data.get('total_items', 10),
            behavior_data.get('document_focus', 0.3),
            behavior_data.get('financial_focus', 0.3),
            len(user_items)
        ])
        
        # 3. íŠ¹ì„± ê²°í•© ë° ì •ê·œí™”
        if len(text_features) < 10:
            text_features = np.pad(text_features, (0, 10 - len(text_features)))
        
        combined_features = np.concatenate([text_features[:10], behavior_features])
        return self.scaler.fit_transform(combined_features.reshape(1, -1))[0]
    
    def fit_user_clusters(self, user_profiles: List[np.ndarray]) -> 'UserBehaviorAnalyzer':
        """ì‚¬ìš©ì í´ëŸ¬ìŠ¤í„° í•™ìŠµ"""
        
        if len(user_profiles) < 5:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ìƒì„±
            print("âš ï¸ ì‚¬ìš©ì ë°ì´í„° ë¶€ì¡± - ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ì‚¬ìš©")
            return self
            
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ
        X = np.array(user_profiles)
        X_pca = self.pca.fit_transform(X)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        self.kmeans.fit(X_pca)
        
        print(f"âœ… ì‚¬ìš©ì í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(user_profiles)}ëª… â†’ {self.kmeans.n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        return self
    
    def predict_user_type(self, user_profile: np.ndarray) -> Tuple[int, str, float]:
        """ì‚¬ìš©ì ìœ í˜• ì˜ˆì¸¡"""
        
        try:
            # PCA ë³€í™˜ í›„ í´ëŸ¬ìŠ¤í„° ì˜ˆì¸¡
            user_pca = self.pca.transform(user_profile.reshape(1, -1))
            cluster_id = self.kmeans.predict(user_pca)[0]
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
            distances = self.kmeans.transform(user_pca)[0]
            confidence = 1.0 / (1.0 + distances[cluster_id])
            
            user_type = self.user_types.get(cluster_id, "ì¼ë°˜í˜•")
            
            return cluster_id, user_type, confidence
            
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì ìœ í˜• ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return 1, "ê³„íší˜• (Steady Planner)", 0.5

class AdvancedRecommendationEngine:
    """ê³ ë„í™”ëœ ì¶”ì²œ ì—”ì§„"""
    
    def __init__(self):
        self.behavior_analyzer = UserBehaviorAnalyzer()
        self.item_embeddings = {}
        self.user_clusters = {}
        
    def extract_item_features(self, popularity_data: List[dict]) -> Dict[str, np.ndarray]:
        """í•­ëª©ë³„ íŠ¹ì„± ë²¡í„° ì¶”ì¶œ (TF-IDF + LSA)"""
        
        # 1. í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
        texts = [f"{item['item_title']} {item['item_description']}" for item in popularity_data]
        
        # TF-IDF ë²¡í„°í™”
        tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words='english')
        tfidf_matrix = tfidf_vectorizer.fit_transform(texts)
        
        # LSA (Latent Semantic Analysis) ì ìš©
        lsa = LatentDirichletAllocation(n_components=10, random_state=42)
        lsa_features = lsa.fit_transform(tfidf_matrix)
        
        # 2. í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ
        item_features = {}
        for i, item in enumerate(popularity_data):
            
            # í…ìŠ¤íŠ¸ íŠ¹ì„± (LSA)
            text_features = lsa_features[i]
            
            # í†µê³„ì  íŠ¹ì„±
            stat_features = np.array([
                float(item['popularity_rate']),
                float(item['priority_score']) / 10.0,
                abs(item['avg_offset_days']) / 100.0,  # ì •ê·œí™”
                1.0 if item['item_tag'] == 'DOCUMENT' else 0.0,
                1.0 if item['item_tag'] == 'EXCHANGE' else 0.0,
                1.0 if item['item_tag'] == 'INSURANCE' else 0.0
            ])
            
            # íŠ¹ì„± ê²°í•©
            combined = np.concatenate([text_features, stat_features])
            item_features[item['item_title']] = combined
            
        return item_features
    
    def find_similar_users_advanced(self, 
                                  target_user_profile: np.ndarray,
                                  all_user_profiles: List[Tuple[int, np.ndarray]],
                                  top_k: int = 5) -> List[int]:
        """PCA + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ìœ ì‚¬ ì‚¬ìš©ì ê²€ìƒ‰"""
        
        if len(all_user_profiles) < 2:
            return []
            
        # ì‚¬ìš©ì í”„ë¡œí•„ë“¤ì„ í–‰ë ¬ë¡œ ë³€í™˜
        user_ids = [user_id for user_id, _ in all_user_profiles]
        profiles = np.array([profile for _, profile in all_user_profiles])
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=min(10, profiles.shape[1]))
        profiles_pca = pca.fit_transform(profiles)
        target_pca = pca.transform(target_user_profile.reshape(1, -1))
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(target_pca, profiles_pca)[0]
        
        # ìƒìœ„ Këª… ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        similar_user_ids = [user_ids[i] for i in top_indices if similarities[i] > 0.1]
        
        return similar_user_ids
    
    def recommend_with_clustering(self,
                                existing_items: List[dict],
                                user_behavior: dict,
                                popularity_data: List[dict],
                                all_user_data: List[dict] = None) -> List[dict]:
        """í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ê³ ë„í™”ëœ ì¶”ì²œ"""
        
        try:
            # 1. ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
            user_profile = self.behavior_analyzer.create_user_profile(existing_items, user_behavior)
            
            # 2. ì‚¬ìš©ì ìœ í˜• ë¶„ì„
            cluster_id, user_type, confidence = self.behavior_analyzer.predict_user_type(user_profile)
            
            print(f"ğŸ¤– ì‚¬ìš©ì ìœ í˜• ë¶„ì„: {user_type} (ì‹ ë¢°ë„: {confidence:.2f})")
            
            # 3. í•­ëª© íŠ¹ì„± ì¶”ì¶œ
            item_features = self.extract_item_features(popularity_data)
            
            # 4. ê¸°ì¡´ í•­ëª©ë“¤ì˜ í‰ê·  íŠ¹ì„± ë²¡í„°
            existing_titles = {item['title'].lower() for item in existing_items}
            existing_features = []
            
            for title, features in item_features.items():
                if title.lower() in existing_titles:
                    existing_features.append(features)
            
            if existing_features:
                user_preference_vector = np.mean(existing_features, axis=0)
            else:
                user_preference_vector = np.zeros(16)  # ê¸°ë³¸ ë²¡í„°
            
            # 5. ì¶”ì²œ ì ìˆ˜ ê³„ì‚° (ê³ ë„í™”)
            recommendations = []
            for item in popularity_data:
                item_title = item['item_title'].lower()
                
                # ì´ë¯¸ ìˆëŠ” í•­ëª© ì œì™¸
                if item_title in existing_titles:
                    continue
                
                # ê¸°ë³¸ ì ìˆ˜
                base_score = float(item['popularity_rate']) * 0.4
                
                # ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
                type_bonus = self._get_type_bonus(cluster_id, item)
                
                # íŠ¹ì„± ìœ ì‚¬ë„ ì ìˆ˜
                if item['item_title'] in item_features:
                    item_feature = item_features[item['item_title']]
                    similarity = cosine_similarity(
                        user_preference_vector.reshape(1, -1),
                        item_feature.reshape(1, -1)
                    )[0][0]
                    similarity_score = max(0, similarity) * 0.3
                else:
                    similarity_score = 0.1
                
                # ìµœì¢… ì ìˆ˜
                final_score = base_score + type_bonus + similarity_score
                
                recommendations.append({
                    **item,
                    'ai_score': final_score,
                    'user_type': user_type,
                    'type_bonus': type_bonus,
                    'similarity_score': similarity_score,
                    'recommendation_reason': self._get_recommendation_reason(cluster_id, type_bonus, similarity_score)
                })
            
            # 6. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ 5ê°œ ë°˜í™˜
            recommendations.sort(key=lambda x: x['ai_score'], reverse=True)
            return recommendations[:5]
            
        except Exception as e:
            print(f"âŒ ê³ ë„í™” ì¶”ì²œ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì¶”ì²œ ë¡œì§
            return self._fallback_recommendation(existing_items, popularity_data)
    
    def _get_type_bonus(self, cluster_id: int, item: dict) -> float:
        """ì‚¬ìš©ì ìœ í˜•ë³„ ë³´ë„ˆìŠ¤ ì ìˆ˜"""
        
        item_tag = item['item_tag']
        popularity = float(item['popularity_rate'])
        
        # ìœ í˜•ë³„ ì„ í˜¸ë„
        type_preferences = {
            0: {'DOCUMENT': 0.3, 'INSURANCE': 0.2},  # ì™„ë²½ì£¼ì˜ì
            1: {'DOCUMENT': 0.2, 'ETC': 0.1},        # ê³„íší˜•  
            2: {'EXCHANGE': 0.2, 'ETC': 0.3},        # ë§‰íŒìŠ¤íŒŒíŠ¸
            3: {'INSURANCE': 0.3, 'DOCUMENT': 0.2},  # ì‹ ì¤‘í˜•
            4: {'EXCHANGE': 0.2, 'ETC': 0.1}         # ì‹¤ìš©ì£¼ì˜ì
        }
        
        preference = type_preferences.get(cluster_id, {})
        return preference.get(item_tag, 0.0) + (popularity * 0.1)
    
    def _get_recommendation_reason(self, cluster_id: int, type_bonus: float, similarity_score: float) -> str:
        """ì¶”ì²œ ì´ìœ  ìƒì„±"""
        
        if type_bonus > 0.2:
            return f"ğŸ¯ {self.behavior_analyzer.user_types[cluster_id]} ìœ í˜•ì—ê²Œ ì¸ê¸°"
        elif similarity_score > 0.2:
            return "ğŸ” ì„ í˜¸ íŒ¨í„´ê³¼ ìœ ì‚¬í•œ í•­ëª©"
        else:
            return "ğŸ“Š ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì¤€ë¹„í•˜ëŠ” í•­ëª©"
    
    def _fallback_recommendation(self, existing_items: List[dict], popularity_data: List[dict]) -> List[dict]:
        """í´ë°± ì¶”ì²œ (ê¸°ë³¸ ë¡œì§)"""
        existing_titles = {item['title'].lower() for item in existing_items}
        
        recommendations = []
        for item in popularity_data:
            if item['item_title'].lower() not in existing_titles:
                recommendations.append({
                    **item,
                    'ai_score': float(item['popularity_rate']),
                    'recommendation_reason': "ğŸ“Š ê¸°ë³¸ ì¶”ì²œ"
                })
        
        recommendations.sort(key=lambda x: x['ai_score'], reverse=True)
        return recommendations[:5]

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_advanced_recommendations():
    """ê³ ë„í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    # ìƒ˜í”Œ ë°ì´í„°
    existing_items = [
        {'title': 'ì—¬ê¶Œ ë°œê¸‰', 'description': '6ê°œì›” ì´ìƒ ìœ íš¨ê¸°ê°„ í™•ì¸', 'tag': 'DOCUMENT'},
        {'title': 'í•­ê³µê¶Œ ì˜ˆì•½', 'description': 'ì™•ë³µ í•­ê³µê¶Œ', 'tag': 'ETC'}
    ]
    
    user_behavior = {
        'completion_rate': 0.8,
        'avg_days_early': 15,
        'total_items': 12,
        'document_focus': 0.6,
        'financial_focus': 0.2
    }
    
    popularity_data = [
        {
            'item_title': 'F-1 í•™ìƒë¹„ì ë°œê¸‰',
            'item_description': 'DS-160 ì‘ì„± ë° ì˜ì‚¬ê´€ ì¸í„°ë·°',
            'item_tag': 'DOCUMENT',
            'popularity_rate': 0.95,
            'avg_offset_days': -90,
            'priority_score': 1
        },
        {
            'item_title': 'ë‹¬ëŸ¬ í™˜ì „',
            'item_description': 'í˜„ì§€ ìƒí™œë¹„ 3-6ê°œì›”ë¶„',
            'item_tag': 'EXCHANGE',
            'popularity_rate': 0.88,
            'avg_offset_days': -21,
            'priority_score': 2
        }
    ]
    
    # ì¶”ì²œ ì—”ì§„ ì‹¤í–‰
    engine = AdvancedRecommendationEngine()
    recommendations = engine.recommend_with_clustering(
        existing_items, user_behavior, popularity_data
    )
    
    print("ğŸš€ ê³ ë„í™”ëœ ì¶”ì²œ ê²°ê³¼:")
    for rec in recommendations:
        print(f"  â€¢ {rec['item_title']} (ì ìˆ˜: {rec.get('ai_score', 0):.3f})")
        print(f"    â†’ {rec.get('recommendation_reason', 'ì¼ë°˜ ì¶”ì²œ')}")

if __name__ == "__main__":
    test_advanced_recommendations()